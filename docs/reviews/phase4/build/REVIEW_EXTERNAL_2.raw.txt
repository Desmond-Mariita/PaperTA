{"type":"result","subtype":"success","is_error":false,"duration_ms":64449,"duration_api_ms":64400,"num_turns":12,"result":"---\n\n# Phase 4 Build — External Review Report\n\n## Summary\n\nPhase 4 delivers a multi-paper intelligence pipeline (`multi_paper` mode) that orchestrates per-paper ingestion/retrieval, concept linking to canonical IDs, cross-paper consensus labeling, and cross-paper graph construction. The implementation is structurally sound: frozen dataclass contracts, deterministic tokenization, validated failure modes, and aggregate observability counters. All 37 tests pass and the checklist scores 10/10.\n\nHowever, there are meaningful gaps in test coverage for contract invariants, a naming drift between the ADR and the implementation, and one logic concern in the consensus classifier.\n\n---\n\n## Findings\n\n### MAJOR-01: Consensus label logic can produce \"contradicting\" for benign text containing common English fragments\n\n**Severity: MAJOR**\n\n**File:** `src/paperta/multi_paper.py:114`\n\n```python\ntexts = \" \".join(hit.text.lower() for hit in all_hits)\nif \" no \" in f\" {texts} \" or \" not \" in f\" {texts} \":\n    labels.append(\"contradicting\")\n```\n\nThe substring heuristic triggers on incidental occurrences of \"no\" and \"not\" that do not indicate contradiction. For example, the text `\"This is not uncommon in NLP\"` (a double-negative meaning agreement) or `\"There is no doubt this approach works\"` would be classified as `\"contradicting\"`, which is semantically wrong.\n\nThe ADR acknowledges this is heuristic (`\"Contradiction detection remains heuristic in core phase\"`), but the current implementation lacks even basic guard-rails (e.g., checking bigrams or requiring \"no\" to precede a content word). This directly violates Contract Invariant #2 (\"Consensus labels are restricted to declared enum values\" — the label is valid syntactically but semantically misleading) and Non-Functional Requirement #1 (Determinism is preserved, but correctness is not).\n\n**Recommendation:** At minimum, add a code comment documenting known false-positive cases, and consider a more targeted heuristic or a configurable flag to disable contradiction detection.\n\n---\n\n### MAJOR-02: Missing test for empty query and top_k <= 0 validation paths\n\n**Severity: MAJOR**\n\n**File:** `tests/negative/test_phase4_multi_paper_negative.py`\n\nThe runtime contract (`PHASE4_RUNTIME_CONTRACT.md`) declares four failure modes:\n\n| Failure Mode | Tested? |\n|---|---|\n| Empty paper set | Yes (`test_phase4_pipeline_rejects_empty_paper_set`) |\n| Invalid mode | Yes (`test_phase4_pipeline_rejects_invalid_mode`) |\n| Empty query | **No** |\n| `top_k <= 0` | **No** |\n\n`src/paperta/multi_paper.py:200-203` validates both:\n```python\nif not query.strip():\n    raise ValueError(\"query must be non-empty\")\nif top_k <= 0:\n    raise ValueError(\"top_k must be > 0\")\n```\n\nBut no negative test exercises these paths. The contract explicitly lists them as failure modes. The acceptance checklist (T4-NEG-001, T4-NEG-002) does not cover them either.\n\n**Recommendation:** Add `test_phase4_pipeline_rejects_empty_query` and `test_phase4_pipeline_rejects_invalid_top_k` to the negative test file, and add corresponding checklist items.\n\n---\n\n### MAJOR-03: No test for cross-paper graph construction (`build_cross_paper_graph`)\n\n**Severity: MAJOR**\n\n**File:** `tests/unit/test_multi_paper.py`\n\nThe contract lists `build_cross_paper_graph` as a core interface (Interface #4). The unit tests cover `link_concepts` (T4-UNIT-001) and `build_consensus_matrix` (T4-UNIT-002) but completely skip `build_cross_paper_graph`. The integration test asserts `graph_edge_count >= 1` but does not verify edge structure, relation types, or evidence anchors.\n\nContract requirement #4: \"Cross-paper graph includes paper-concept and paper-paper relationship edges with evidence anchors\" is not directly verified.\n\n**Recommendation:** Add a dedicated unit test `test_build_cross_paper_graph_emits_expected_edges` that verifies both `mentions` and `shares_concept` edge types, validates evidence_chunk_ids are non-empty for non-fallback cases, and checks source/target alignment.\n\n---\n\n### MINOR-01: ADR names don't match implementation structure\n\n**Severity: MINOR**\n\n**File:** `docs/adr/ADR-004-phase4-multi-paper-intelligence.md:15-17`\n\nThe ADR refers to service classes:\n```\n1. ConceptLinkerService\n2. ConsensusService\n3. CrossPaperGraphService\n4. MultiPaperPipeline\n```\n\nThe implementation uses free functions instead:\n```python\nlink_concepts(...)\nbuild_consensus_matrix(...)\nbuild_cross_paper_graph(...)\nrun_phase4_multi_paper_pipeline(...)\n```\n\nThis is a reasonable simplification, but the ADR should be updated to reflect the actual design to avoid confusion during future reviews.\n\n---\n\n### MINOR-02: `link_concepts` only uses the first retrieval hit per paper\n\n**Severity: MINOR**\n\n**File:** `src/paperta/multi_paper.py:75`\n\n```python\nhit = result.retrieval_trace.hits[0]\nlocal_name = f\"{hit.section} concept\"\n```\n\nOnly the first hit is used to derive the local concept. If a paper has multiple relevant hits across sections, subsequent concepts are silently dropped. This is undocumented in the contract and limits the richness of concept linking.\n\n**Recommendation:** Document this single-concept-per-paper behavior in the contract, or extend to extract concepts from all hits.\n\n---\n\n### MINOR-03: `evidence_chunk_ids` silently truncated to 4 in consensus matrix\n\n**Severity: MINOR**\n\n**File:** `src/paperta/multi_paper.py:128`\n\n```python\nevidence_chunk_ids=tuple(hit.chunk_id for hit in all_hits[:4]),\n```\n\nThe hard limit of 4 evidence chunks is undocumented in the contract. For a pipeline with many papers, this could silently discard evidence.\n\n**Recommendation:** Document the limit in the contract or make it configurable.\n\n---\n\n### MINOR-04: `link_by_paper` dict in `build_cross_paper_graph` silently deduplicates papers with same ID\n\n**Severity: MINOR**\n\n**File:** `src/paperta/multi_paper.py:147`\n\n```python\nlink_by_paper = {link.paper_id: link for link in concept_links.concepts}\n```\n\nIf two `LocalConcept` entries share the same `paper_id` (which shouldn't happen given current `link_concepts` logic, but is not contract-enforced), the later entry silently overwrites the earlier. There's no validation that `paper_id` is unique across concepts.\n\n---\n\n### NIT-01: `_concept_id` truncates to first 3 tokens — collision risk undocumented\n\n**Severity: NIT**\n\n**File:** `src/paperta/multi_paper.py:50`\n\n```python\nstem = \"_\".join(tokens[:3]) if tokens else \"unknown\"\nreturn f\"g_{stem}\"\n```\n\nFor concepts like \"Intro concept\" and \"Intro concept extended\", both map to `g_intro_concept`. This is correct by design (canonical grouping) but the collision behavior is not documented.\n\n---\n\n### NIT-02: Integration test does not assert `unsupported_entry_count`\n\n**Severity: NIT**\n\n**File:** `tests/integration/test_phase4_multi_paper_pipeline.py:22-26`\n\nThe test asserts `mode`, `paper_count`, `consensus_claim_count`, `graph_edge_count`, and `per_paper_retrieval` but never checks `unsupported_entry_count`. The contract lists this as an observability field.\n\n---\n\n## Open Questions\n\n1. **Scalability intent:** The current pairwise graph comparison (`zip(paper_results, paper_results[1:])` at `multi_paper.py:160`) only compares adjacent papers in input order. Is this intentional (O(n) edges) or should all paper pairs be compared (O(n²) edges)? For 2 papers this is identical, but for 3+ papers it produces different graphs depending on input order, which conflicts with the determinism requirement if paper ordering is not guaranteed.\n\n2. **Concept granularity:** The contract says \"local concept entries\" (plural) but the implementation emits exactly one concept per paper. Should this be clarified as a known simplification?\n\n3. **Duplicate paper_id handling:** What should happen if two `PaperInput` objects share the same `paper_id`? The current code would ingest both but `link_by_paper` would deduplicate silently. Should this be a validation error?\n\n---\n\n## Final Verdict\n\n**CONDITIONAL PASS — 3 major findings must be addressed before merge.**\n\nThe architecture and contracts are well-structured. The deterministic design, frozen dataclasses, and clear separation between contracts and implementation follow established Phase 1-3 patterns. However:\n\n1. **MAJOR-01** (consensus false positives) — At minimum document the known limitation inline and in the contract. A follow-up ticket for improved heuristics is acceptable.\n2. **MAJOR-02** (missing negative tests for empty query / invalid top_k) — Add the two missing negative tests. These are trivial to write and close a contract-test gap.\n3. **MAJOR-03** (no unit test for graph builder) — Add a targeted unit test for `build_cross_paper_graph` covering both edge types.\n\nOnce these three are resolved, the phase clears the external review gate.","stop_reason":null,"session_id":"aa5cc335-79d9-4f0b-a821-7a4973e64067","total_cost_usd":0.21665474999999998,"usage":{"input_tokens":5,"cache_creation_input_tokens":13819,"cache_read_input_tokens":104022,"output_tokens":3130,"server_tool_use":{"web_search_requests":0,"web_fetch_requests":0},"service_tier":"standard","cache_creation":{"ephemeral_1h_input_tokens":13819,"ephemeral_5m_input_tokens":0},"inference_geo":"","iterations":[],"speed":"standard"},"modelUsage":{"claude-opus-4-6":{"inputTokens":5,"outputTokens":3130,"cacheReadInputTokens":104022,"cacheCreationInputTokens":13819,"webSearchRequests":0,"costUSD":0.21665474999999998,"contextWindow":200000,"maxOutputTokens":32000}},"permission_denials":[],"uuid":"3b67c1c5-30b6-4a38-a172-f3b75ada56f6"}
