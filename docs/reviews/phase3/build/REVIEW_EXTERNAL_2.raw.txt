{"type":"result","subtype":"success","is_error":false,"duration_ms":53897,"duration_api_ms":53853,"num_turns":10,"result":"I now have all artifacts. Here is my full review.\n\n---\n\n# Phase 3 Build — External Review Report\n\n## Summary\n\nPhase 3 delivers a deterministic Reviewer Mode with three sub-artifacts (critique, reproducibility checklist, claim-evidence matrix) built on the Phase 1 ingestion/retrieval substrate. The code is well-structured: all dataclasses are frozen/immutable, the pipeline enforces mode constraints and input validation, and fallback semantics use the contractually required sentinel text. Tests span unit, integration, and negative categories.\n\nHowever, the review identifies one **MAJOR** logic defect in the `unsupported_claim_count` computation that silently produces incorrect observability counters, one **MAJOR** gap in test coverage for the `generate_critique` function, and several minor issues.\n\n---\n\n## Findings\n\n### MAJOR-01: `unsupported_claim_count` uses wrong predicate — counts \"Not stated\" claims instead of unsupported-graded claims\n\n**File:** `src/paperta/reviewer.py:229`\n\n```python\nunsupported_claim_count = sum(1 for row in claim_matrix.rows if row.claim == NOT_STATED)\n```\n\nThe field is named `unsupported_claim_count` and the contract (PHASE3_RUNTIME_CONTRACT.md, Observability §1) lists it as an aggregate reviewer counter. The natural reading — and the one most useful for auditing — is \"number of claims whose `support_grade` is `unsupported`\". But the implementation counts rows where the **claim text** equals the fallback sentinel `\"Not stated in the paper.\"`.\n\nThese two predicates diverge whenever the retrieval set is non-empty: `generate_claim_evidence_matrix` (line 177) assigns grade `\"supported\"` or `\"mixed\"` — never `\"unsupported\"` — to every real hit, so `unsupported_claim_count` will always be **0** for any non-empty retrieval. Meanwhile the sentinel-text check only matches the single fallback row produced when there are zero hits. In the zero-hits case both predicates coincidentally agree (one fallback row with grade `\"unsupported\"` and text `NOT_STATED`), masking the bug.\n\n**Impact:** Any consumer relying on `unsupported_claim_count` to surface claims with weak evidence will get a misleading 0 in all normal-retrieval scenarios. If the grading logic later allows `\"unsupported\"` for real hits, the counter will still be wrong because it checks text, not grade.\n\n**Suggested fix:**\n```python\nunsupported_claim_count = sum(1 for row in claim_matrix.rows if row.support_grade == \"unsupported\")\n```\n\n**Severity:** MAJOR\n\n---\n\n### MAJOR-02: No unit test for `generate_critique`\n\n**File:** `tests/unit/test_reviewer.py` (entire file)\n\nThe unit test file covers `generate_claim_evidence_matrix` and `generate_reproducibility_checklist` but has **no test** for `generate_critique`. The contract (PHASE3_RUNTIME_CONTRACT.md, Interfaces §2) lists `generate_critique` as an explicit public interface. The integration test exercises it indirectly but does not assert on its structure (strengths/weaknesses/threats fields, evidence anchors, fallback behavior).\n\nThe acceptance checklist (`PHASE3_ACCEPTANCE_CHECKLIST.yaml`) also lacks a checklist item for a critique unit test, so the gap is systemic — the checklist itself failed to require it.\n\n**Impact:** Regressions in critique generation logic (fallback path, snippet truncation, field assignment) would go undetected.\n\n**Severity:** MAJOR\n\n---\n\n### MINOR-01: Dead-code validation in `generate_claim_evidence_matrix`\n\n**File:** `src/paperta/reviewer.py:178-179`\n\n```python\nif grade not in _VALID_SUPPORT_GRADES:\n    raise ValueError(\"invalid support grade\")\n```\n\nThe variable `grade` is assigned on line 177 by a ternary that can only produce `\"supported\"` or `\"mixed\"`, both members of `_VALID_SUPPORT_GRADES`. The guard is unreachable. While defensive coding is reasonable, unreachable branches add noise and give a false sense of validation. If the intent is to guard against future grading changes, a comment would clarify that.\n\n**Severity:** MINOR\n\n---\n\n### MINOR-02: Reproducibility status values not validated by contract\n\n**File:** `src/paperta/reviewer.py:134`, `docs/contracts/PHASE3_RUNTIME_CONTRACT.md`\n\nThe reproducibility checklist uses statuses `\"pass\"`, `\"warning\"`, and `\"not_stated\"` (lines 119, 134). These are asserted in the unit test (`test_reviewer.py:33`), but the **runtime contract** never enumerates the valid set of reproducibility statuses. The contract specifies valid values for support grades (Invariants §2) but omits the equivalent for reproducibility status.\n\n**Impact:** Consumers have no contractual guarantee of the value domain; only the test documents it.\n\n**Severity:** MINOR\n\n---\n\n### MINOR-03: No negative test for invalid `mode` or `top_k <= 0`\n\n**Files:** `tests/negative/test_phase3_reviewer_negative.py`, `docs/contracts/PHASE3_RUNTIME_CONTRACT.md`\n\nThe contract lists \"Invalid mode -> ValueError\" (Failure Mode §4) and \"top_k <= 0 -> ValueError\" (Failure Mode §2) as explicit failure modes. Neither has a corresponding negative test. Only the empty-query case is tested.\n\n**Severity:** MINOR\n\n---\n\n### NIT-01: `ReproducibilityItem.status` could use a constrained type\n\n**File:** `src/paperta/reviewer_contracts.py:33`\n\n`status: str` is unconstrained. A `Literal[\"pass\", \"warning\", \"not_stated\"]` annotation would make the contract self-documenting and catch misuse statically. Same applies to `ClaimEvidenceRow.support_grade` (line 50).\n\n**Severity:** NIT\n\n---\n\n### NIT-02: Snippet function named `_snippet` but not tested in isolation\n\n**File:** `src/paperta/reviewer.py:25-37`\n\n`_snippet` has boundary behavior (exact-length equality, truncation with rstrip + ellipsis). It is exercised indirectly through integration tests, but boundary cases (e.g., text exactly at `max_len`, text at `max_len - 1`, whitespace at truncation boundary) are untested.\n\n**Severity:** NIT\n\n---\n\n## Open Questions\n\n1. **Is `unsupported_claim_count` intended to count claims with `support_grade == \"unsupported\"` or claims with fallback text?** The name and contract phrasing suggest the former. If the latter is intentional, the field should be renamed to something like `not_stated_claim_count` for clarity.\n\n2. **Should the checklist enumerate valid reproducibility statuses** (like it does for support grades) as a contract invariant, or is this intentionally left open for future extension?\n\n3. **Is there a reason `generate_critique` was excluded from unit testing?** The other two sub-artifact generators each have a dedicated unit test.\n\n---\n\n## Final Verdict\n\n**CONDITIONAL PASS** — Two MAJOR issues must be resolved before merge:\n\n1. Fix the `unsupported_claim_count` predicate to check `row.support_grade == \"unsupported\"` (or rename/redocument if the current behavior is intentional).\n2. Add a dedicated unit test for `generate_critique` covering both the happy path and the empty-retrieval fallback path, and add the corresponding checklist item.\n\nThe MINOR issues (contract gap for reproducibility statuses, missing negative tests for mode/top_k) are recommended but not blocking.","stop_reason":null,"session_id":"eb53e8b5-9d4d-40c4-93ae-53def21172f3","total_cost_usd":0.174148,"usage":{"input_tokens":4,"cache_creation_input_tokens":11760,"cache_read_input_tokens":77956,"output_tokens":2466,"server_tool_use":{"web_search_requests":0,"web_fetch_requests":0},"service_tier":"standard","cache_creation":{"ephemeral_1h_input_tokens":11760,"ephemeral_5m_input_tokens":0},"inference_geo":"","iterations":[],"speed":"standard"},"modelUsage":{"claude-opus-4-6":{"inputTokens":4,"outputTokens":2466,"cacheReadInputTokens":77956,"cacheCreationInputTokens":11760,"webSearchRequests":0,"costUSD":0.174148,"contextWindow":200000,"maxOutputTokens":32000}},"permission_denials":[],"uuid":"6ff7b7e8-90df-40cc-b687-49fe0d65c017"}
