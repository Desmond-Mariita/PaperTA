# External Review (claude)

- Generated at: 2026-02-17T10:17:38+00:00
- Command: `claude -p --output-format json --dangerously-skip-permissions`
- Input mode: `stdin`
- Exit code: 0
- Raw file: `REVIEW_EXTERNAL_2.raw.txt`
- Raw SHA256: `cc64d66637a22b92fb471abe13c246ad5d5839c17f811bc59b4ab7700705c3c2`

## Raw Output

```text
{"type":"result","subtype":"success","is_error":false,"duration_ms":34072,"duration_api_ms":34041,"num_turns":1,"result":"\n\n# Phase 2 Design Review — External\n\n## Summary\n\nThe Phase 2 design artifacts (ADR-002, Runtime Contract, Acceptance Checklist) describe a deterministic Teach Mode pipeline built atop Phase 1's ingestion and retrieval layer. The artifacts are well-structured, internally consistent, and exhibit strong traceability discipline (citation chunk IDs, evidence anchors, retrieval traces). The design is conservative and appropriately scoped. A small number of gaps and inconsistencies are identified below.\n\n---\n\n## Findings\n\n### MAJOR\n\n**M1 — Contract interface omits `generate_explanation` but checklist and contract body require it**\n\nThe contract lists five generation functions:\n\n> `generate_prerequisites`, `generate_concept_map`, `generate_explanation`, `generate_quiz`, `answer_socratic_question`\n\nHowever, the acceptance checklist contains **no test item** for `generate_explanation`. There are unit test items for prerequisites (`T2-UNIT-001`) and concept map (`T2-UNIT-002`), but no `T2-UNIT-003` for explanation generation. The contract's Functional Requirement #4 explicitly states:\n\n> \"Step-by-step explanation contains ordered steps with citation chunk IDs.\"\n\nThis is a testable, grounded output with its own interface, yet it has zero test coverage in the checklist. This creates a gap where the explanation service could ship untested.\n\n**Severity: MAJOR** — A documented interface and functional requirement has no acceptance gate.\n\n---\n\n**M2 — Quiz generation has no dedicated unit test in the checklist**\n\nSimilarly, `generate_quiz` is a first-class interface in the contract (Functional Requirement #5):\n\n> \"Quiz includes MCQ and short-answer items with answer key and evidence anchors.\"\n\nAnd the contract Invariant #3 states:\n\n> \"Quiz answer keys must correspond to generated options/prompts.\"\n\nYet there is no `T2-UNIT-003` or equivalent for quiz generation unit testing. The integration test `T2-INT-001` covers the pipeline end-to-end, but a unit-level test for quiz answer-key/option correspondence and evidence anchor validity is absent from the checklist.\n\n**Severity: MAJOR** — A critical invariant (answer keys match options) lacks a dedicated unit-level acceptance gate.\n\n---\n\n### MINOR\n\n**m1 — ADR alternatives section has a grammatical/wording issue**\n\n> \"Rejected due weak auditability and unclear evidence linking.\"\n\nShould read \"Rejected **due to** weak auditability…\". File: ADR-002, Alternatives Considered, item 1.\n\n**Severity: MINOR** — Cosmetic, but ADRs are archival documents and should be precise.\n\n---\n\n**m2 — Failure Mode #5 semantics are ambiguous for structured outputs**\n\nThe contract states:\n\n> \"Retrieval empty set -> teach artifacts degrade gracefully with `Not stated in the paper.` for unsupported fields.\"\n\nIt is unclear what \"unsupported fields\" means in the context of structured outputs like `ConceptMap` (nodes/edges) or `Quiz` (items/options). Does an empty retrieval produce an empty concept map, or a concept map with a single node whose label is `Not stated in the paper.`? The graceful-degradation contract for each structured artifact type should be specified individually.\n\n**Severity: MINOR** — Ambiguity that will likely surface as an implementation question in the build loop.\n\n---\n\n**m3 — Checklist has no gate for `answer_socratic_question` unit test**\n\n`T2-NEG-002` covers the socratic fallback (negative path), but there is no positive-path unit test item verifying that `answer_socratic_question` returns a correct, evidence-grounded answer when the question *is* supported. Only the negative case is gated.\n\n**Severity: MINOR** — Negative test alone is insufficient to confirm the happy-path contract for this interface.\n\n---\n\n### NIT\n\n**n1 — Checklist version and phase metadata lack schema reference**\n\nThe checklist opens with:\n\n```yaml\nversion: \"1.0\"\nphase: 2\n```\n\nThere is no `$schema` or format reference. Phase 1's checklist presumably uses the same structure, but cross-phase consistency is not enforceable without a shared schema definition.\n\n**Severity: NIT**\n\n---\n\n**n2 — Observability contract field `unsupported_response_count` naming**\n\nThe field `unsupported_response_count` could be confused with HTTP-level unsupported responses. A name like `not_stated_fallback_count` would be more self-documenting and consistent with the exact fallback text `Not stated in the paper.`.\n\n**Severity: NIT**\n\n---\n\n## Open Questions\n\n1. **Explanation and Quiz unit tests**: Was the omission of dedicated unit-test checklist items for `generate_explanation` and `generate_quiz` intentional (relying on integration coverage), or an oversight? If intentional, should the rationale be documented?\n\n2. **Empty retrieval degradation shape**: What is the expected structure of each teach artifact (concept map, quiz, explanation) when retrieval returns zero chunks? Should the contract specify empty-collection vs. sentinel-value behavior per artifact type?\n\n3. **Socratic context window**: The contract specifies `answer_socratic_question(ingested_paper, question, retrieval_result)`. Is the full `ingested_paper` passed to bound context size, or is it expected to be pre-filtered? This has implications for determinism if truncation is involved.\n\n4. **Adaptive difficulty deferral**: The ADR notes adaptive difficulty is deferred. Is there a placeholder or extension point anticipated in the data model (e.g., a `difficulty` field on quiz items), or is this entirely out of scope for Phase 2 schemas?\n\n---\n\n## Final Verdict\n\n**CONDITIONAL PASS** — The design is sound and well-traced, but two **MAJOR** findings must be resolved before the build loop begins:\n\n1. Add acceptance checklist items for `generate_explanation` unit test coverage.\n2. Add acceptance checklist items for `generate_quiz` unit test coverage (specifically validating answer-key/option correspondence and evidence anchors).\n\nOnce these gaps are closed, the design is ready to proceed to the Phase 2 build loop.","stop_reason":null,"session_id":"9f49a611-4dac-4355-a2b6-a05fbe863f42","total_cost_usd":0.0682855,"usage":{"input_tokens":2,"cache_creation_input_tokens":3330,"cache_read_input_tokens":21676,"output_tokens":1465,"server_tool_use":{"web_search_requests":0,"web_fetch_requests":0},"service_tier":"standard","cache_creation":{"ephemeral_1h_input_tokens":3330,"ephemeral_5m_input_tokens":0},"inference_geo":"","iterations":[],"speed":"standard"},"modelUsage":{"claude-opus-4-6":{"inputTokens":2,"outputTokens":1465,"cacheReadInputTokens":21676,"cacheCreationInputTokens":3330,"webSearchRequests":0,"costUSD":0.0682855,"contextWindow":200000,"maxOutputTokens":32000}},"permission_denials":[],"uuid":"9b289c58-6e1d-4c54-af48-3f593f2cbcb8"}
```
